{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sylvain/Library/Mobile Documents/com~apple~CloudDocs/Projects/deeplearning ai short courses/.venvPinecone/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all packaged imported\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "_ = load_dotenv(find_dotenv(\"secrets.env\", raise_error_if_not_found=True))\n",
    "\n",
    "ROOT_DIR = os.environ[\"ROOT_DIR\"]\n",
    "print(\"all packaged imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Transform the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the truth of life?\n",
      "What's the evil truth of life?\n",
      "Which is the best smartphone under 20K in India?\n",
      "Which is the best smartphone with in 20k in India?\n",
      "Steps taken by Canadian government to improve literacy rate?\n",
      "--------------------------------------------------\n",
      "Number of questions = 100041\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "with open(file=f\"{ROOT_DIR}/Pinecone Building Applications with Vector Databases/files/datasetText.txt\") as file:\n",
    "\tdataset = file.read().split(\"\\n\")[1:]\n",
    "# dataset[:20]\n",
    "\n",
    "# Extract the questions from the dataset\n",
    "import re\n",
    "questions = []\n",
    "for data in dataset:\n",
    "\tresult = re.findall(r\"'text': \\[([^\\]]+)\", data)\n",
    "\tresult = list(result)\n",
    "\tfor q in result:\n",
    "\t# \tq = q.split(\", \")\n",
    "\t\tpattern = r'([\\'\"])(.*?)\\1'\n",
    "\t\tq = re.findall(pattern, q)\n",
    "\t\tfor e in q:\n",
    "\t\t\tgarbage, question = e\n",
    "\t\t\tquestions.append(question)\n",
    "\n",
    "print(\"\\n\".join(questions[:5]))\n",
    "print(\"-\"*50)\n",
    "print(f\"Number of questions = {len(questions)}\")\n",
    "\n",
    "# Not exactly 100000 questions, but that should do it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: all-MiniLM-L6-v2\n",
      "Running on: mps\n"
     ]
    }
   ],
   "source": [
    "# See if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cude\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Load the model on the GPU\n",
    "modelName = \"all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(model_name_or_path=modelName, device=device)\n",
    "print(f\"Model: {modelName}\\nRunning on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of xq: (384,)\n",
      "Type of xq: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the most populated city in the world?\"\n",
    "xq = model.encode(sentences=query)\n",
    "print(f\"Shape of xq: {xq.shape}\\nType of xq: {type(xq)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': [{'dimension': 384,\n",
       "              'host': 'pinecone-deeplearning-ai-skrwyc8.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'pinecone-deeplearning-ai',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up Pinecone\n",
    "PINECONE_API_KEY = os.environ[\"PINECONE_API_KEY\"]\n",
    "pinecone = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Create an Index\n",
    "INDEX_NAME = \"pinecone-deeplearning-ai\"\n",
    "\n",
    "# check if it already exists\n",
    "# if INDEX_NAME in pinecone.list_indexes():\n",
    "# \tpinecone.delete_index(INDEX_NAME)\n",
    "\n",
    "# # create the index\n",
    "# pinecone.create_index(\n",
    "# \tname=INDEX_NAME,\n",
    "# \tdimension=model.get_sentence_embedding_dimension(),\n",
    "# \tspec=ServerlessSpec(cloud=\"aws\", region=\"us-west-2\"), #TODO see if they have a region in Europe\n",
    "# \tmetric=\"cosine\",\n",
    "# )\n",
    "\n",
    "# Instantiate the index object\n",
    "index = pinecone.Index(INDEX_NAME)\n",
    "\n",
    "# Show the list of all existing indexes associated with this API key\n",
    "pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vector representation of the questions and store them in the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 vectors have been stored into the index\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 10000}},\n",
       " 'total_vector_count': 10000}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 200\n",
    "vector_limit = 10000\n",
    "questionsVec = questions[:vector_limit]\n",
    "\n",
    "for i in tqdm(range(0, vector_limit, batch_size)):\n",
    "\n",
    "\t# def iEnd\n",
    "\tiEnd = min(i + batch_size, vector_limit)\n",
    "\n",
    "\t# def ids for this batch\n",
    "\tids = [str(x) for x in range(i, iEnd)]\n",
    "\n",
    "\t# prepare the metadata (here: the questions)\n",
    "\tmetadata = [{\"text\": text} for text in questionsVec[i:iEnd]]\n",
    "\n",
    "\t# create the emeddings\n",
    "\temb = model.encode(questionsVec[i:iEnd])\n",
    "\n",
    "\t# zip ids, metadata and embeddings together\n",
    "\tpackage = zip(ids, emb, metadata)\n",
    "\n",
    "\t# store this batch info into the index\n",
    "\tindex.upsert(vectors=package)\n",
    "\n",
    "print(f\"{vector_limit} vectors have been stored into the index\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# see the details of our index\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94: What are the best movies of all time?\n",
      "0.87: Which is the all time best Hollywood movie?\n",
      "0.83: Which is best Hollywood movie ever? Why?\n",
      "0.8: What is best movies till date?\n",
      "0.79: What are the best Hollywood movies ever?\n",
      "0.78: Which is the best hollywood movie you have seen?\n",
      "0.78: What are some of the best movies of all times I should watch (animated are welcome)?\n",
      "0.77: Which is the best movie ever in Hollywood(new)?\n",
      "0.77: Which is the best movie ever in Hollywood(new)?\n",
      "0.71: What are your favorite movies and why?\n"
     ]
    }
   ],
   "source": [
    "# helper function to run queries\n",
    "def runQuery(query, top_k = 10):\n",
    "\temb = model.encode(query).tolist()\n",
    "\tresults = index.query(vector=emb, top_k=top_k, include_metadata=True, include_values=False)\n",
    "\tfor result in results[\"matches\"]:\n",
    "\t\tprint(f\"{round(result['score'], 2)}: {result['metadata']['text']}\")\n",
    "\n",
    "# define your query\n",
    "query = \"what is the best movie of all times?\"\n",
    "\n",
    "# see the results of the semantic search\n",
    "runQuery(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScienceVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
