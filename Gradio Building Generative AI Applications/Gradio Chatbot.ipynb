{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatBot app with Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import gradio as gr\n",
    "import openai\n",
    "\n",
    "_ = load_dotenv(find_dotenv(filename=\"secrets.env\", raise_error_if_not_found=False))\n",
    "\n",
    "# Global variable\n",
    "ROOT_DIR = os.environ[\"ROOT_DIR\"]\n",
    "SYSTEM_PROMPT = \"\\\n",
    "\tYou are a helpful assistant and do your best to answer the user's questions.\\\n",
    "\tYou do not make up answers.\"\n",
    "AUTH_USERNAME = os.environ[\"AUTH_USERNAME\"]\n",
    "AUTH_PASSWORD = os.environ[\"AUTH_PASSWORD\"]\n",
    "\n",
    "# Load credential for API calls\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and test the API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello\n",
      "!\n",
      " As\n",
      " an\n",
      " AI\n",
      ",\n",
      " I\n",
      " don\n",
      "'t\n",
      " have\n",
      " feelings\n",
      ",\n",
      " but\n",
      " I\n",
      "'m\n",
      " here\n",
      " and\n",
      " ready\n",
      " to\n",
      " assist\n",
      " you\n",
      ".\n",
      " How\n",
      " can\n",
      " I\n",
      " help\n",
      " you\n",
      " today\n",
      "?\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define the function that will make the API calls \n",
    "def APIcall(prompt:str, temperature = 0.7, max_tokens = 1024, model=\"GPT-3.5\", stream=True):\n",
    "\tif model == \"GPT-3.5\":\n",
    "\t\tmodel = \"gpt-3.5-turbo\"\n",
    "\telse:\n",
    "\t\tmodel = \"gpt-4\"\n",
    "\t# make the API call with the given parameter\n",
    "\tresponse = openai.chat.completions.create(\n",
    "\t\tmodel=model,\n",
    "\t\tmessages=prompt,\n",
    "\t\tmax_tokens = max_tokens,\n",
    "\t\ttemperature=temperature,\n",
    "\t\tstream=stream,\n",
    "\t)\n",
    "\n",
    "\t# return the completed text\n",
    "\tif stream:\n",
    "\t\tfor chunk in response:\n",
    "\t\t\toutput = chunk.choices[0].delta.content # when Stream is set to True\n",
    "\t\t\tyield output\n",
    "\telse:\n",
    "\t\toutput = response.choices[0].message.content # when Stream is set to False\n",
    "\n",
    "# Format a prompt to test the API\n",
    "prompt = [\n",
    "\t{\n",
    "\t\"role\": \"system\",\n",
    "\t\"content\": SYSTEM_PROMPT\n",
    "\t},\n",
    "\t{\n",
    "\t\t\"role\": \"user\",\n",
    "\t\t\"content\": \"Hi, how are you today?\"\n",
    "\t}]\n",
    "\n",
    "# test it\n",
    "for token in APIcall(prompt):\n",
    "\tprint(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the ChatBot with Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helper function: format the prompt to include history\n",
    "def formatPrompt(newMsg:str, chatHistory, instruction):\n",
    "\t\n",
    "\t# start with the system prompt\n",
    "\tmessages = []\n",
    "\tmessages.append({\n",
    "\t\t\"role\": \"system\",\n",
    "\t\t\"content\": instruction\n",
    "\t})\n",
    "\n",
    "\t# add the history\n",
    "\tfor turn in chatHistory:\n",
    "\t\t# retrieve the user and assistant messages from history\n",
    "\t\tuserMsg, AssistantMsg = turn\n",
    "\t\t\n",
    "\t\t# add the user message\n",
    "\t\tmessages.append({\n",
    "\t\t\t\"role\": \"user\",\n",
    "\t\t\t\"content\": userMsg\n",
    "\t\t})\n",
    "\n",
    "\t\t# add the assistant message\n",
    "\t\tmessages.append({\n",
    "\t\t\t\"role\": \"assistant\",\n",
    "\t\t\t\"content\": AssistantMsg\n",
    "\t\t})\n",
    "\t\n",
    "\t# add the last message that needs to be answer\n",
    "\tmessages.append({\n",
    "\t\t\"role\": \"user\",\n",
    "\t\t\"content\": newMsg\n",
    "\t})\n",
    "\n",
    "\t# return the formated messages\n",
    "\treturn messages\n",
    "\n",
    "# def the response function (to get the answer as one block after generation)\n",
    "def response(newMsg:str, chatHistory, instruction, temperature, max_tokens, model, stream=False):\n",
    "\tprompt = formatPrompt(newMsg=newMsg, chatHistory=chatHistory, instruction=instruction)\n",
    "\tresponse = APIcall(prompt=prompt, temperature=temperature, max_tokens=max_tokens, model=model)\n",
    "\tchatHistory.append([newMsg, response])\n",
    "\treturn \"\", chatHistory\n",
    "\n",
    "# def the streamResponse function, to stream the results as they are generated\n",
    "def streamResponse(newMsg:str, chatHistory, instruction, temperature, max_tokens, model, stream = True):\n",
    "\tchatHistory.append([newMsg, \"\"])\n",
    "\tprompt = formatPrompt(newMsg=newMsg, chatHistory=chatHistory, instruction=instruction)\n",
    "\tstream = APIcall(prompt=prompt, temperature=temperature, max_tokens=max_tokens, model=model)\n",
    "\tfor chunk in stream:\n",
    "\t\tif chunk != None:\n",
    "\t\t\tchatHistory[-1][1] += chunk\n",
    "\t\t\tyield \"\", chatHistory\n",
    "\t\telse:\n",
    "\t\t\treturn \"\", chatHistory\n",
    "\n",
    "# Build the app\n",
    "with gr.Blocks(theme='Insuz/Mocha') as app:\n",
    "\tgr.Markdown(\"# Private GPT\")\n",
    "\tgr.Markdown(\"This chatbot is powered by the openAI GPT series.\\\n",
    "\t\t\t \\nThe default model is `GPT-3.5`, but `GPT-4` can be selected in the advanced options.\\\n",
    "\t\t\t \\nAs it uses the openAI API, user data is not used to train openAI models.\")\n",
    "\tchatbot = gr.Chatbot() # Associated variable: chatHistory\n",
    "\tmsg = gr.Textbox(label=\"Message\")\n",
    "\twith gr.Accordion(label=\"Advanced options\", open=False):\n",
    "\t\tmodel = gr.Dropdown(\n",
    "\t\t\tchoices=[\"GPT-3.5\", \"GPT-4\"],\n",
    "\t\t\tvalue=\"GPT-3.5\",\n",
    "\t\t\tmultiselect=False,\n",
    "\t\t\tlabel=\"Model\",\n",
    "\t\t\tinfo=\"Choose the model you want to chat with\"\n",
    "\t\t)\n",
    "\t\tinstruction = gr.Textbox(\n",
    "\t\t\tvalue=SYSTEM_PROMPT,\n",
    "\t\t\tlabel=\"System instructions\",\n",
    "\t\t\tlines=2,)\n",
    "\t\ttemperature = gr.Slider(\n",
    "\t\t\tminimum=0,\n",
    "\t\t\tmaximum=2,\n",
    "\t\t\tstep=0.1,\n",
    "\t\t\tvalue=0.7,\n",
    "\t\t\tlabel=\"Temperature\",\n",
    "\t\t\tinfo=\"The higher, the more random the results will be\"\n",
    "\t\t)\n",
    "\t\tmax_token = gr.Slider(\n",
    "\t\t\tminimum=64,\n",
    "\t\t\tmaximum=2048,\n",
    "\t\t\tstep=64,\n",
    "\t\t\tvalue=1024,\n",
    "\t\t\tlabel=\"Max Token\",\n",
    "\t\t\tinfo=\"Maximum number of token the model will take into consideration\"\n",
    "\t\t)\n",
    "\tButton = gr.Button(value=\"Submit\")\n",
    "\tmsg.submit(\n",
    "\t\tfn=streamResponse,\n",
    "\t\tinputs=[msg, chatbot, instruction, temperature, max_token, model],\n",
    "\t\toutputs=[msg, chatbot]\n",
    "\t)\n",
    "\tButton.click(\n",
    "\t\tfn=streamResponse,\n",
    "\t\tinputs=[msg, chatbot, instruction, temperature, max_token, model],\n",
    "\t\toutputs=[msg, chatbot]\n",
    "\t)\n",
    "\n",
    "gr.close_all()\n",
    "app.queue().launch(auth=(AUTH_USERNAME, AUTH_PASSWORD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScienceVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
